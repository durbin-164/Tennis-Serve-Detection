{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Jo8GKA_am93w",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1703612023835,
     "user_tz": -360,
     "elapsed": 7319,
     "user": {
      "displayName": "Md. Masud Rana",
      "userId": "04170274789345913474"
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, LayerNormalization\n",
    "import numpy as np\n",
    "# from keras.utils import np_utils\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "_SWtRaCXdrI0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1703620766932,
     "user_tz": -360,
     "elapsed": 337,
     "user": {
      "displayName": "Md. Masud Rana",
      "userId": "04170274789345913474"
     }
    }
   },
   "outputs": [],
   "source": [
    "not_serve_path = '/content/all_pose_nor_not_serve_real_32_v2.npy'\n",
    "serve_path = '/content/all_pose_nor_serve_real_32_v2.npy'\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "\n",
    "# not_serve_data = np.load(not_serve_path)\n",
    "# serve_data = np.load(serve_path)\n",
    "\n",
    "# all_data = np.concatenate((not_serve_data, serve_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "E-D5jl2QddP8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1703620768431,
     "user_tz": -360,
     "elapsed": 311,
     "user": {
      "displayName": "Md. Masud Rana",
      "userId": "04170274789345913474"
     }
    }
   },
   "outputs": [],
   "source": [
    "# not_serve_label = np.zeros((not_serve_data.shape[0]), dtype=\"int\").reshape(-1,1)\n",
    "# serve_label = np.ones((serve_data.shape[0]), dtype=\"int\").reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "kl_Qu2oIddSi",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1703620768967,
     "user_tz": -360,
     "elapsed": 2,
     "user": {
      "displayName": "Md. Masud Rana",
      "userId": "04170274789345913474"
     }
    }
   },
   "outputs": [],
   "source": [
    "# labels = np.vstack([not_serve_label, serve_label])\n",
    "\n",
    "# y= ohe.fit_transform(labels).toarray().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "tj4HQ_6DddU6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1703620769286,
     "user_tz": -360,
     "elapsed": 2,
     "user": {
      "displayName": "Md. Masud Rana",
      "userId": "04170274789345913474"
     }
    }
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(all_data,y, test_size = 0.20, shuffle= True)\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n",
    "\n",
    "# print(X_test.shape)\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "zYxjRS3uLnGq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1703620769286,
     "user_tz": -360,
     "elapsed": 2,
     "user": {
      "displayName": "Md. Masud Rana",
      "userId": "04170274789345913474"
     }
    },
    "outputId": "d0e94f96-4de8-4c86-b2ba-cdee244a6599"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(254, 32, 51)\n",
      "(64, 32, 51)\n"
     ]
    }
   ],
   "source": [
    "not_serve_data = np.load(not_serve_path)\n",
    "not_serve_label = np.zeros((not_serve_data.shape[0]), dtype=\"int\").reshape(-1,1)\n",
    "\n",
    "X_train_not_serve, X_test_not_serve, y_train_not_serve, y_test_not_serve = train_test_split(not_serve_data,not_serve_label, test_size = 0.20, shuffle= True)\n",
    "\n",
    "print(X_train_not_serve.shape)\n",
    "print(X_test_not_serve.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "2ppO621ULnLh",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1703620770834,
     "user_tz": -360,
     "elapsed": 3,
     "user": {
      "displayName": "Md. Masud Rana",
      "userId": "04170274789345913474"
     }
    },
    "outputId": "4566665d-003c-4186-87dc-ce3bc5befac2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(298, 32, 51)\n",
      "(75, 32, 51)\n"
     ]
    }
   ],
   "source": [
    "serve_data = np.load(serve_path)\n",
    "serve_label = np.ones((serve_data.shape[0]), dtype=\"int\").reshape(-1,1)\n",
    "\n",
    "X_train_serve, X_test_serve, y_train_serve, y_test_serve = train_test_split(serve_data,serve_label, test_size = 0.20, shuffle= True)\n",
    "\n",
    "print(X_train_serve.shape)\n",
    "print(X_test_serve.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 343,
     "status": "ok",
     "timestamp": 1703620777990,
     "user": {
      "displayName": "Md. Masud Rana",
      "userId": "04170274789345913474"
     },
     "user_tz": -360
    },
    "id": "nlQPU8SFLnNY",
    "outputId": "f570e6d6-fd46-476e-b271-c667b679f8e3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(552, 32, 51)\n",
      "(552, 2)\n",
      "(139, 32, 51)\n",
      "(139, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.concatenate((X_train_not_serve, X_train_serve))\n",
    "\n",
    "X_test = np.concatenate((X_test_not_serve, X_test_serve))\n",
    "\n",
    "y_train = np.vstack([y_train_not_serve, y_train_serve])\n",
    "y_test = np.vstack([y_test_not_serve, y_test_serve])\n",
    "\n",
    "y_train= ohe.fit_transform(y_train).toarray().astype(int)\n",
    "y_test= ohe.fit_transform(y_test).toarray().astype(int)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Vtl88o-MWvT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 1246,
     "status": "ok",
     "timestamp": 1703620807317,
     "user": {
      "displayName": "Md. Masud Rana",
      "userId": "04170274789345913474"
     },
     "user_tz": -360
    },
    "id": "0MMw9Zj65Ba8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.4):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming your input shape is (24, 51)\n",
    "input_shape = (32, 51)\n",
    "embed_dim = 51  # Adjust based on your needs\n",
    "num_heads = 4\n",
    "ff_dim =32\n",
    "num_transformer_blocks = 4\n",
    "mlp_dim = 32\n",
    "num_classes = 2  # Adjust based on the number of activity classes\n",
    "\n",
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "# x = layers.Flatten()(inputs)  # Flatten the input to fit into the Transformer model\n",
    "x = inputs\n",
    "for i in range(num_transformer_blocks):\n",
    "    # print(i)\n",
    "    x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dense(mlp_dim, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "x = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Define learning rate decay schedule\n",
    "initial_learning_rate = 1e-4\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=1000, decay_rate=0.9, staircase=True\n",
    ")\n",
    "\n",
    "# Compile the model with the optimizer and learning rate decay\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PSTrl9iULKwQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1703621282841,
     "user_tz": -360,
     "elapsed": 474313,
     "user": {
      "displayName": "Md. Masud Rana",
      "userId": "04170274789345913474"
     }
    },
    "outputId": "2a51a39a-f01f-4c4c-cb7f-c5c4f83702e9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6991 - accuracy: 0.5562\n",
      "Epoch 1: val_accuracy improved from -inf to 0.60432, saving model to lstm_model_32_s2_v1_5.keras\n",
      "18/18 [==============================] - 13s 190ms/step - loss: 0.6991 - accuracy: 0.5562 - val_loss: 0.6808 - val_accuracy: 0.6043\n",
      "Epoch 2/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.6574 - accuracy: 0.5993\n",
      "Epoch 2: val_accuracy improved from 0.60432 to 0.61871, saving model to lstm_model_32_s2_v1_5.keras\n",
      "18/18 [==============================] - 2s 130ms/step - loss: 0.6603 - accuracy: 0.5942 - val_loss: 0.6543 - val_accuracy: 0.6187\n",
      "Epoch 3/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.6385 - accuracy: 0.6489\n",
      "Epoch 3: val_accuracy improved from 0.61871 to 0.69784, saving model to lstm_model_32_s2_v1_5.keras\n",
      "18/18 [==============================] - 3s 149ms/step - loss: 0.6402 - accuracy: 0.6467 - val_loss: 0.6419 - val_accuracy: 0.6978\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5950 - accuracy: 0.7101\n",
      "Epoch 4: val_accuracy did not improve from 0.69784\n",
      "18/18 [==============================] - 3s 197ms/step - loss: 0.5950 - accuracy: 0.7101 - val_loss: 0.6219 - val_accuracy: 0.6978\n",
      "Epoch 5/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.5661 - accuracy: 0.7224\n",
      "Epoch 5: val_accuracy improved from 0.69784 to 0.72662, saving model to lstm_model_32_s2_v1_5.keras\n",
      "18/18 [==============================] - 2s 134ms/step - loss: 0.5645 - accuracy: 0.7246 - val_loss: 0.6115 - val_accuracy: 0.7266\n",
      "Epoch 6/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.5514 - accuracy: 0.7316\n",
      "Epoch 6: val_accuracy improved from 0.72662 to 0.74101, saving model to lstm_model_32_s2_v1_5.keras\n",
      "18/18 [==============================] - 2s 113ms/step - loss: 0.5511 - accuracy: 0.7301 - val_loss: 0.5661 - val_accuracy: 0.7410\n",
      "Epoch 7/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.5285 - accuracy: 0.7371\n",
      "Epoch 7: val_accuracy improved from 0.74101 to 0.75540, saving model to lstm_model_32_s2_v1_5.keras\n",
      "18/18 [==============================] - 2s 138ms/step - loss: 0.5270 - accuracy: 0.7391 - val_loss: 0.5364 - val_accuracy: 0.7554\n",
      "Epoch 8/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.5110 - accuracy: 0.7684\n",
      "Epoch 8: val_accuracy did not improve from 0.75540\n",
      "18/18 [==============================] - 2s 118ms/step - loss: 0.5121 - accuracy: 0.7681 - val_loss: 0.5286 - val_accuracy: 0.7554\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5043 - accuracy: 0.7844\n",
      "Epoch 9: val_accuracy improved from 0.75540 to 0.78417, saving model to lstm_model_32_s2_v1_5.keras\n",
      "18/18 [==============================] - 2s 138ms/step - loss: 0.5043 - accuracy: 0.7844 - val_loss: 0.5038 - val_accuracy: 0.7842\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.4858 - accuracy: 0.7699\n",
      "Epoch 10: val_accuracy did not improve from 0.78417\n",
      "18/18 [==============================] - 3s 173ms/step - loss: 0.4858 - accuracy: 0.7699 - val_loss: 0.5282 - val_accuracy: 0.7626\n",
      "Epoch 11/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.5043 - accuracy: 0.7684\n",
      "Epoch 11: val_accuracy did not improve from 0.78417\n",
      "18/18 [==============================] - 2s 105ms/step - loss: 0.5030 - accuracy: 0.7681 - val_loss: 0.4761 - val_accuracy: 0.7842\n",
      "Epoch 12/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.4569 - accuracy: 0.7978\n",
      "Epoch 12: val_accuracy improved from 0.78417 to 0.79856, saving model to lstm_model_32_s2_v1_5.keras\n",
      "18/18 [==============================] - 2s 111ms/step - loss: 0.4580 - accuracy: 0.7953 - val_loss: 0.4820 - val_accuracy: 0.7986\n",
      "Epoch 13/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.4484 - accuracy: 0.7886\n",
      "Epoch 13: val_accuracy did not improve from 0.79856\n",
      "18/18 [==============================] - 2s 103ms/step - loss: 0.4487 - accuracy: 0.7880 - val_loss: 0.4434 - val_accuracy: 0.7986\n",
      "Epoch 14/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.4251 - accuracy: 0.8143\n",
      "Epoch 14: val_accuracy did not improve from 0.79856\n",
      "18/18 [==============================] - 2s 128ms/step - loss: 0.4245 - accuracy: 0.8134 - val_loss: 0.3992 - val_accuracy: 0.7986\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.4163 - accuracy: 0.8188\n",
      "Epoch 15: val_accuracy improved from 0.79856 to 0.81295, saving model to lstm_model_32_s2_v1_5.keras\n",
      "18/18 [==============================] - 3s 156ms/step - loss: 0.4163 - accuracy: 0.8188 - val_loss: 0.4070 - val_accuracy: 0.8129\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.3931 - accuracy: 0.8315\n",
      "Epoch 16: val_accuracy did not improve from 0.81295\n",
      "18/18 [==============================] - 3s 172ms/step - loss: 0.3931 - accuracy: 0.8315 - val_loss: 0.4580 - val_accuracy: 0.8058\n",
      "Epoch 17/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.3924 - accuracy: 0.8290\n",
      "Epoch 17: val_accuracy did not improve from 0.81295\n",
      "18/18 [==============================] - 2s 108ms/step - loss: 0.3937 - accuracy: 0.8279 - val_loss: 0.4106 - val_accuracy: 0.8129\n",
      "Epoch 18/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.3710 - accuracy: 0.8474\n",
      "Epoch 18: val_accuracy improved from 0.81295 to 0.83453, saving model to lstm_model_32_s2_v1_5.keras\n",
      "18/18 [==============================] - 2s 116ms/step - loss: 0.3765 - accuracy: 0.8460 - val_loss: 0.3774 - val_accuracy: 0.8345\n",
      "Epoch 19/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.3504 - accuracy: 0.8511\n",
      "Epoch 19: val_accuracy did not improve from 0.83453\n",
      "18/18 [==============================] - 2s 129ms/step - loss: 0.3520 - accuracy: 0.8514 - val_loss: 0.4025 - val_accuracy: 0.8201\n",
      "Epoch 20/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.3518 - accuracy: 0.8529\n",
      "Epoch 20: val_accuracy improved from 0.83453 to 0.86331, saving model to lstm_model_32_s2_v1_5.keras\n",
      "18/18 [==============================] - 2s 124ms/step - loss: 0.3525 - accuracy: 0.8514 - val_loss: 0.3770 - val_accuracy: 0.8633\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.3467 - accuracy: 0.8641\n",
      "Epoch 21: val_accuracy did not improve from 0.86331\n",
      "18/18 [==============================] - 3s 158ms/step - loss: 0.3467 - accuracy: 0.8641 - val_loss: 0.3891 - val_accuracy: 0.8561\n",
      "Epoch 22/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.3272 - accuracy: 0.8805\n",
      "Epoch 22: val_accuracy did not improve from 0.86331\n",
      "18/18 [==============================] - 3s 179ms/step - loss: 0.3287 - accuracy: 0.8804 - val_loss: 0.4250 - val_accuracy: 0.8273\n",
      "Epoch 23/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.3079 - accuracy: 0.8768\n",
      "Epoch 23: val_accuracy did not improve from 0.86331\n",
      "18/18 [==============================] - 2s 115ms/step - loss: 0.3058 - accuracy: 0.8786 - val_loss: 0.4155 - val_accuracy: 0.8633\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.3009 - accuracy: 0.8859\n",
      "Epoch 24: val_accuracy improved from 0.86331 to 0.89928, saving model to lstm_model_32_s2_v1_5.keras\n",
      "18/18 [==============================] - 2s 133ms/step - loss: 0.3009 - accuracy: 0.8859 - val_loss: 0.3205 - val_accuracy: 0.8993\n",
      "Epoch 25/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2611 - accuracy: 0.9044\n",
      "Epoch 25: val_accuracy did not improve from 0.89928\n",
      "18/18 [==============================] - 2s 131ms/step - loss: 0.2591 - accuracy: 0.9058 - val_loss: 0.4290 - val_accuracy: 0.8633\n",
      "Epoch 26/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2885 - accuracy: 0.8952\n",
      "Epoch 26: val_accuracy improved from 0.89928 to 0.91367, saving model to lstm_model_32_s2_v1_5.keras\n",
      "18/18 [==============================] - 3s 143ms/step - loss: 0.2916 - accuracy: 0.8913 - val_loss: 0.2806 - val_accuracy: 0.9137\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.2523 - accuracy: 0.9094\n",
      "Epoch 27: val_accuracy did not improve from 0.91367\n",
      "18/18 [==============================] - 3s 194ms/step - loss: 0.2523 - accuracy: 0.9094 - val_loss: 0.3055 - val_accuracy: 0.9065\n",
      "Epoch 28/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2628 - accuracy: 0.9118\n",
      "Epoch 28: val_accuracy did not improve from 0.91367\n",
      "18/18 [==============================] - 2s 97ms/step - loss: 0.2653 - accuracy: 0.9094 - val_loss: 0.3840 - val_accuracy: 0.8921\n",
      "Epoch 29/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2238 - accuracy: 0.9246\n",
      "Epoch 29: val_accuracy did not improve from 0.91367\n",
      "18/18 [==============================] - 2s 118ms/step - loss: 0.2275 - accuracy: 0.9239 - val_loss: 0.3064 - val_accuracy: 0.9137\n",
      "Epoch 30/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2659 - accuracy: 0.8915\n",
      "Epoch 30: val_accuracy did not improve from 0.91367\n",
      "18/18 [==============================] - 2s 101ms/step - loss: 0.2632 - accuracy: 0.8931 - val_loss: 0.4013 - val_accuracy: 0.8921\n",
      "Epoch 31/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2317 - accuracy: 0.9118\n",
      "Epoch 31: val_accuracy did not improve from 0.91367\n",
      "18/18 [==============================] - 2s 108ms/step - loss: 0.2293 - accuracy: 0.9130 - val_loss: 0.4761 - val_accuracy: 0.8849\n",
      "Epoch 32/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2164 - accuracy: 0.9210\n",
      "Epoch 32: val_accuracy did not improve from 0.91367\n",
      "18/18 [==============================] - 2s 108ms/step - loss: 0.2159 - accuracy: 0.9203 - val_loss: 0.3893 - val_accuracy: 0.8849\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.2049 - accuracy: 0.9221\n",
      "Epoch 33: val_accuracy did not improve from 0.91367\n",
      "18/18 [==============================] - 3s 161ms/step - loss: 0.2049 - accuracy: 0.9221 - val_loss: 0.3596 - val_accuracy: 0.8921\n",
      "Epoch 34/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2044 - accuracy: 0.9357\n",
      "Epoch 34: val_accuracy did not improve from 0.91367\n",
      "18/18 [==============================] - 3s 139ms/step - loss: 0.2027 - accuracy: 0.9366 - val_loss: 0.4146 - val_accuracy: 0.8849\n",
      "Epoch 35/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2181 - accuracy: 0.9320\n",
      "Epoch 35: val_accuracy did not improve from 0.91367\n",
      "18/18 [==============================] - 2s 115ms/step - loss: 0.2162 - accuracy: 0.9330 - val_loss: 0.3259 - val_accuracy: 0.9137\n",
      "Epoch 36/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1849 - accuracy: 0.9301\n",
      "Epoch 36: val_accuracy did not improve from 0.91367\n",
      "18/18 [==============================] - 2s 110ms/step - loss: 0.1830 - accuracy: 0.9312 - val_loss: 0.3782 - val_accuracy: 0.8993\n",
      "Epoch 37/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1752 - accuracy: 0.9412\n",
      "Epoch 37: val_accuracy did not improve from 0.91367\n",
      "18/18 [==============================] - 2s 116ms/step - loss: 0.1734 - accuracy: 0.9420 - val_loss: 0.3779 - val_accuracy: 0.8993\n",
      "Epoch 38/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1707 - accuracy: 0.9467\n",
      "Epoch 38: val_accuracy did not improve from 0.91367\n",
      "18/18 [==============================] - 2s 117ms/step - loss: 0.1686 - accuracy: 0.9475 - val_loss: 0.3021 - val_accuracy: 0.9137\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.1529 - accuracy: 0.9420\n",
      "Epoch 39: val_accuracy did not improve from 0.91367\n",
      "18/18 [==============================] - 3s 153ms/step - loss: 0.1529 - accuracy: 0.9420 - val_loss: 0.4531 - val_accuracy: 0.8921\n",
      "Epoch 40/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1787 - accuracy: 0.9393\n",
      "Epoch 40: val_accuracy did not improve from 0.91367\n",
      "18/18 [==============================] - 3s 147ms/step - loss: 0.1796 - accuracy: 0.9384 - val_loss: 0.3669 - val_accuracy: 0.9065\n",
      "Epoch 41/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1756 - accuracy: 0.9283\n",
      "Epoch 41: val_accuracy did not improve from 0.91367\n",
      "18/18 [==============================] - 2s 108ms/step - loss: 0.1770 - accuracy: 0.9275 - val_loss: 0.2624 - val_accuracy: 0.9065\n",
      "Epoch 42/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2250 - accuracy: 0.9099\n",
      "Epoch 42: val_accuracy did not improve from 0.91367\n",
      "18/18 [==============================] - 2s 99ms/step - loss: 0.2242 - accuracy: 0.9094 - val_loss: 0.2951 - val_accuracy: 0.9065\n",
      "Epoch 43/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1698 - accuracy: 0.9357\n",
      "Epoch 43: val_accuracy did not improve from 0.91367\n",
      "18/18 [==============================] - 2s 107ms/step - loss: 0.1690 - accuracy: 0.9366 - val_loss: 0.3135 - val_accuracy: 0.9137\n",
      "Epoch 44/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1543 - accuracy: 0.9540\n",
      "Epoch 44: val_accuracy did not improve from 0.91367\n",
      "18/18 [==============================] - 2s 102ms/step - loss: 0.1529 - accuracy: 0.9547 - val_loss: 0.3392 - val_accuracy: 0.9065\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.1477 - accuracy: 0.9529\n",
      "Epoch 45: val_accuracy did not improve from 0.91367\n",
      "18/18 [==============================] - 2s 115ms/step - loss: 0.1477 - accuracy: 0.9529 - val_loss: 0.2865 - val_accuracy: 0.9137\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.1482 - accuracy: 0.9402\n",
      "Epoch 46: val_accuracy did not improve from 0.91367\n",
      "18/18 [==============================] - 3s 168ms/step - loss: 0.1482 - accuracy: 0.9402 - val_loss: 0.4309 - val_accuracy: 0.8849\n",
      "Epoch 47/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1325 - accuracy: 0.9467\n",
      "Epoch 47: val_accuracy did not improve from 0.91367\n",
      "18/18 [==============================] - 2s 104ms/step - loss: 0.1333 - accuracy: 0.9475 - val_loss: 0.6014 - val_accuracy: 0.8777\n",
      "Epoch 48/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1454 - accuracy: 0.9375\n",
      "Epoch 48: val_accuracy did not improve from 0.91367\n",
      "18/18 [==============================] - 2s 101ms/step - loss: 0.1440 - accuracy: 0.9384 - val_loss: 0.4017 - val_accuracy: 0.9065\n",
      "Epoch 49/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1182 - accuracy: 0.9485\n",
      "Epoch 49: val_accuracy did not improve from 0.91367\n",
      "18/18 [==============================] - 2s 100ms/step - loss: 0.1181 - accuracy: 0.9493 - val_loss: 0.3191 - val_accuracy: 0.9137\n",
      "Epoch 50/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1461 - accuracy: 0.9540\n",
      "Epoch 50: val_accuracy did not improve from 0.91367\n",
      "18/18 [==============================] - 2s 100ms/step - loss: 0.1444 - accuracy: 0.9547 - val_loss: 0.2643 - val_accuracy: 0.9137\n",
      "Epoch 51/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1515 - accuracy: 0.9467\n",
      "Epoch 51: val_accuracy improved from 0.91367 to 0.92086, saving model to lstm_model_32_s2_v1_5.keras\n",
      "18/18 [==============================] - 2s 139ms/step - loss: 0.1504 - accuracy: 0.9475 - val_loss: 0.2847 - val_accuracy: 0.9209\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.1240 - accuracy: 0.9511\n",
      "Epoch 52: val_accuracy did not improve from 0.92086\n",
      "18/18 [==============================] - 3s 166ms/step - loss: 0.1240 - accuracy: 0.9511 - val_loss: 0.3392 - val_accuracy: 0.9137\n",
      "Epoch 53/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0963 - accuracy: 0.9761\n",
      "Epoch 53: val_accuracy did not improve from 0.92086\n",
      "18/18 [==============================] - 3s 155ms/step - loss: 0.0958 - accuracy: 0.9764 - val_loss: 0.5397 - val_accuracy: 0.8921\n",
      "Epoch 54/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1422 - accuracy: 0.9540\n",
      "Epoch 54: val_accuracy did not improve from 0.92086\n",
      "18/18 [==============================] - 2s 131ms/step - loss: 0.1476 - accuracy: 0.9529 - val_loss: 0.5431 - val_accuracy: 0.8705\n",
      "Epoch 55/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1821 - accuracy: 0.9246\n",
      "Epoch 55: val_accuracy did not improve from 0.92086\n",
      "18/18 [==============================] - 2s 101ms/step - loss: 0.1836 - accuracy: 0.9221 - val_loss: 0.3315 - val_accuracy: 0.8993\n",
      "Epoch 56/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1247 - accuracy: 0.9485\n",
      "Epoch 56: val_accuracy did not improve from 0.92086\n",
      "18/18 [==============================] - 2s 111ms/step - loss: 0.1294 - accuracy: 0.9475 - val_loss: 0.3343 - val_accuracy: 0.9137\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.1175 - accuracy: 0.9620\n",
      "Epoch 57: val_accuracy did not improve from 0.92086\n",
      "18/18 [==============================] - 2s 117ms/step - loss: 0.1175 - accuracy: 0.9620 - val_loss: 0.3419 - val_accuracy: 0.9137\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.1573 - accuracy: 0.9493\n",
      "Epoch 58: val_accuracy did not improve from 0.92086\n",
      "18/18 [==============================] - 3s 187ms/step - loss: 0.1573 - accuracy: 0.9493 - val_loss: 0.2969 - val_accuracy: 0.9137\n",
      "Epoch 59/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1197 - accuracy: 0.9632\n",
      "Epoch 59: val_accuracy did not improve from 0.92086\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 0.1223 - accuracy: 0.9601 - val_loss: 0.3803 - val_accuracy: 0.9065\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.1128 - accuracy: 0.9656\n",
      "Epoch 60: val_accuracy did not improve from 0.92086\n",
      "18/18 [==============================] - 2s 117ms/step - loss: 0.1128 - accuracy: 0.9656 - val_loss: 0.5164 - val_accuracy: 0.8921\n",
      "Epoch 61/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0970 - accuracy: 0.9724\n",
      "Epoch 61: val_accuracy did not improve from 0.92086\n",
      "18/18 [==============================] - 2s 117ms/step - loss: 0.0959 - accuracy: 0.9728 - val_loss: 0.3625 - val_accuracy: 0.9065\n",
      "Epoch 62/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1014 - accuracy: 0.9724\n",
      "Epoch 62: val_accuracy did not improve from 0.92086\n",
      "18/18 [==============================] - 2s 102ms/step - loss: 0.1006 - accuracy: 0.9728 - val_loss: 0.3546 - val_accuracy: 0.9065\n",
      "Epoch 63/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0962 - accuracy: 0.9669\n",
      "Epoch 63: val_accuracy did not improve from 0.92086\n",
      "18/18 [==============================] - 2s 111ms/step - loss: 0.0957 - accuracy: 0.9674 - val_loss: 0.4714 - val_accuracy: 0.8993\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0944 - accuracy: 0.9746\n",
      "Epoch 64: val_accuracy did not improve from 0.92086\n",
      "18/18 [==============================] - 3s 165ms/step - loss: 0.0944 - accuracy: 0.9746 - val_loss: 0.4728 - val_accuracy: 0.8993\n",
      "Epoch 65/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1064 - accuracy: 0.9522\n",
      "Epoch 65: val_accuracy improved from 0.92086 to 0.93525, saving model to lstm_model_32_s2_v1_5.keras\n",
      "18/18 [==============================] - 3s 147ms/step - loss: 0.1051 - accuracy: 0.9529 - val_loss: 0.2235 - val_accuracy: 0.9353\n",
      "Epoch 66/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0996 - accuracy: 0.9614\n",
      "Epoch 66: val_accuracy did not improve from 0.93525\n",
      "18/18 [==============================] - 2s 111ms/step - loss: 0.0987 - accuracy: 0.9620 - val_loss: 0.3199 - val_accuracy: 0.9281\n",
      "Epoch 67/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1309 - accuracy: 0.9467\n",
      "Epoch 67: val_accuracy did not improve from 0.93525\n",
      "18/18 [==============================] - 2s 111ms/step - loss: 0.1294 - accuracy: 0.9475 - val_loss: 0.2703 - val_accuracy: 0.9137\n",
      "Epoch 68/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0913 - accuracy: 0.9614\n",
      "Epoch 68: val_accuracy did not improve from 0.93525\n",
      "18/18 [==============================] - 2s 111ms/step - loss: 0.0918 - accuracy: 0.9620 - val_loss: 0.3532 - val_accuracy: 0.9209\n",
      "Epoch 69/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0847 - accuracy: 0.9743\n",
      "Epoch 69: val_accuracy did not improve from 0.93525\n",
      "18/18 [==============================] - 2s 111ms/step - loss: 0.0841 - accuracy: 0.9746 - val_loss: 0.4809 - val_accuracy: 0.8993\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.1155 - accuracy: 0.9674\n",
      "Epoch 70: val_accuracy did not improve from 0.93525\n",
      "18/18 [==============================] - 3s 151ms/step - loss: 0.1155 - accuracy: 0.9674 - val_loss: 0.5141 - val_accuracy: 0.8993\n",
      "Epoch 71/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0931 - accuracy: 0.9632\n",
      "Epoch 71: val_accuracy did not improve from 0.93525\n",
      "18/18 [==============================] - 3s 159ms/step - loss: 0.0933 - accuracy: 0.9638 - val_loss: 0.2938 - val_accuracy: 0.9353\n",
      "Epoch 72/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0817 - accuracy: 0.9743\n",
      "Epoch 72: val_accuracy did not improve from 0.93525\n",
      "18/18 [==============================] - 2s 109ms/step - loss: 0.0875 - accuracy: 0.9728 - val_loss: 0.4044 - val_accuracy: 0.9065\n",
      "Epoch 73/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0871 - accuracy: 0.9706\n",
      "Epoch 73: val_accuracy did not improve from 0.93525\n",
      "18/18 [==============================] - 2s 99ms/step - loss: 0.0862 - accuracy: 0.9710 - val_loss: 0.3550 - val_accuracy: 0.9281\n",
      "Epoch 74/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0687 - accuracy: 0.9816\n",
      "Epoch 74: val_accuracy did not improve from 0.93525\n",
      "18/18 [==============================] - 2s 124ms/step - loss: 0.0687 - accuracy: 0.9819 - val_loss: 0.4114 - val_accuracy: 0.9137\n",
      "Epoch 75/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0788 - accuracy: 0.9761\n",
      "Epoch 75: val_accuracy did not improve from 0.93525\n",
      "18/18 [==============================] - 2s 116ms/step - loss: 0.0792 - accuracy: 0.9746 - val_loss: 0.3902 - val_accuracy: 0.9209\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.1110 - accuracy: 0.9601\n",
      "Epoch 76: val_accuracy did not improve from 0.93525\n",
      "18/18 [==============================] - 3s 144ms/step - loss: 0.1110 - accuracy: 0.9601 - val_loss: 0.6451 - val_accuracy: 0.8849\n",
      "Epoch 77/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1269 - accuracy: 0.9651\n",
      "Epoch 77: val_accuracy did not improve from 0.93525\n",
      "18/18 [==============================] - 3s 165ms/step - loss: 0.1257 - accuracy: 0.9656 - val_loss: 0.4167 - val_accuracy: 0.9209\n",
      "Epoch 78/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0701 - accuracy: 0.9761\n",
      "Epoch 78: val_accuracy did not improve from 0.93525\n",
      "18/18 [==============================] - 2s 103ms/step - loss: 0.0692 - accuracy: 0.9764 - val_loss: 0.3823 - val_accuracy: 0.9137\n",
      "Epoch 79/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0792 - accuracy: 0.9743\n",
      "Epoch 79: val_accuracy did not improve from 0.93525\n",
      "18/18 [==============================] - 2s 118ms/step - loss: 0.0800 - accuracy: 0.9728 - val_loss: 0.4439 - val_accuracy: 0.9065\n",
      "Epoch 80/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0750 - accuracy: 0.9743\n",
      "Epoch 80: val_accuracy did not improve from 0.93525\n",
      "18/18 [==============================] - 2s 112ms/step - loss: 0.0751 - accuracy: 0.9746 - val_loss: 0.4800 - val_accuracy: 0.9065\n",
      "Epoch 81/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0745 - accuracy: 0.9761\n",
      "Epoch 81: val_accuracy did not improve from 0.93525\n",
      "18/18 [==============================] - 2s 102ms/step - loss: 0.0739 - accuracy: 0.9764 - val_loss: 0.3676 - val_accuracy: 0.9209\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9764\n",
      "Epoch 82: val_accuracy did not improve from 0.93525\n",
      "18/18 [==============================] - 2s 135ms/step - loss: 0.0743 - accuracy: 0.9764 - val_loss: 0.3219 - val_accuracy: 0.9281\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9692\n",
      "Epoch 83: val_accuracy did not improve from 0.93525\n",
      "18/18 [==============================] - 3s 186ms/step - loss: 0.0830 - accuracy: 0.9692 - val_loss: 0.2473 - val_accuracy: 0.9353\n",
      "Epoch 84/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0633 - accuracy: 0.9798\n",
      "Epoch 84: val_accuracy did not improve from 0.93525\n",
      "18/18 [==============================] - 2s 103ms/step - loss: 0.0625 - accuracy: 0.9801 - val_loss: 0.3266 - val_accuracy: 0.9281\n",
      "Epoch 85/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0979 - accuracy: 0.9651\n",
      "Epoch 85: val_accuracy did not improve from 0.93525\n",
      "18/18 [==============================] - 2s 104ms/step - loss: 0.0967 - accuracy: 0.9656 - val_loss: 0.4478 - val_accuracy: 0.9137\n",
      "Epoch 86/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0662 - accuracy: 0.9853\n",
      "Epoch 86: val_accuracy did not improve from 0.93525\n",
      "18/18 [==============================] - 2s 102ms/step - loss: 0.0676 - accuracy: 0.9837 - val_loss: 0.4226 - val_accuracy: 0.9209\n",
      "Epoch 87/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0599 - accuracy: 0.9779\n",
      "Epoch 87: val_accuracy did not improve from 0.93525\n",
      "18/18 [==============================] - 2s 102ms/step - loss: 0.0627 - accuracy: 0.9764 - val_loss: 0.3635 - val_accuracy: 0.9281\n",
      "Epoch 88/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0848 - accuracy: 0.9761\n",
      "Epoch 88: val_accuracy did not improve from 0.93525\n",
      "18/18 [==============================] - 2s 113ms/step - loss: 0.0874 - accuracy: 0.9746 - val_loss: 0.4110 - val_accuracy: 0.9209\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9728\n",
      "Epoch 89: val_accuracy did not improve from 0.93525\n",
      "18/18 [==============================] - 3s 159ms/step - loss: 0.0749 - accuracy: 0.9728 - val_loss: 0.4438 - val_accuracy: 0.9209\n",
      "Epoch 90/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0596 - accuracy: 0.9779\n",
      "Epoch 90: val_accuracy improved from 0.93525 to 0.94245, saving model to lstm_model_32_s2_v1_5.keras\n",
      "18/18 [==============================] - 3s 172ms/step - loss: 0.0591 - accuracy: 0.9783 - val_loss: 0.2880 - val_accuracy: 0.9424\n",
      "Epoch 91/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0688 - accuracy: 0.9706\n",
      "Epoch 91: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 2s 104ms/step - loss: 0.0688 - accuracy: 0.9710 - val_loss: 0.4602 - val_accuracy: 0.9209\n",
      "Epoch 92/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0511 - accuracy: 0.9908\n",
      "Epoch 92: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 2s 124ms/step - loss: 0.0567 - accuracy: 0.9891 - val_loss: 0.2793 - val_accuracy: 0.9424\n",
      "Epoch 93/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0493 - accuracy: 0.9798\n",
      "Epoch 93: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 2s 105ms/step - loss: 0.0490 - accuracy: 0.9801 - val_loss: 0.3139 - val_accuracy: 0.9353\n",
      "Epoch 94/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0492 - accuracy: 0.9798\n",
      "Epoch 94: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 2s 103ms/step - loss: 0.0493 - accuracy: 0.9801 - val_loss: 0.3496 - val_accuracy: 0.9281\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0878 - accuracy: 0.9692\n",
      "Epoch 95: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 3s 166ms/step - loss: 0.0878 - accuracy: 0.9692 - val_loss: 0.4189 - val_accuracy: 0.9209\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0506 - accuracy: 0.9873\n",
      "Epoch 96: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 3s 159ms/step - loss: 0.0506 - accuracy: 0.9873 - val_loss: 0.4248 - val_accuracy: 0.9281\n",
      "Epoch 97/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0399 - accuracy: 0.9890\n",
      "Epoch 97: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 2s 116ms/step - loss: 0.0394 - accuracy: 0.9891 - val_loss: 0.3465 - val_accuracy: 0.9353\n",
      "Epoch 98/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0503 - accuracy: 0.9798\n",
      "Epoch 98: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 2s 121ms/step - loss: 0.0525 - accuracy: 0.9783 - val_loss: 0.3187 - val_accuracy: 0.9424\n",
      "Epoch 99/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0588 - accuracy: 0.9798\n",
      "Epoch 99: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 2s 114ms/step - loss: 0.0584 - accuracy: 0.9801 - val_loss: 0.4268 - val_accuracy: 0.9281\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.1442 - accuracy: 0.9547\n",
      "Epoch 100: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 2s 125ms/step - loss: 0.1442 - accuracy: 0.9547 - val_loss: 0.3223 - val_accuracy: 0.9281\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9783\n",
      "Epoch 101: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 3s 187ms/step - loss: 0.0700 - accuracy: 0.9783 - val_loss: 0.3679 - val_accuracy: 0.9353\n",
      "Epoch 102/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0674 - accuracy: 0.9724\n",
      "Epoch 102: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 2s 135ms/step - loss: 0.0666 - accuracy: 0.9728 - val_loss: 0.2946 - val_accuracy: 0.9281\n",
      "Epoch 103/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0579 - accuracy: 0.9816\n",
      "Epoch 103: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 2s 102ms/step - loss: 0.0598 - accuracy: 0.9801 - val_loss: 0.3972 - val_accuracy: 0.9281\n",
      "Epoch 104/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0503 - accuracy: 0.9816\n",
      "Epoch 104: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 2s 117ms/step - loss: 0.0500 - accuracy: 0.9819 - val_loss: 0.3426 - val_accuracy: 0.9353\n",
      "Epoch 105/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0522 - accuracy: 0.9835\n",
      "Epoch 105: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 2s 103ms/step - loss: 0.0550 - accuracy: 0.9801 - val_loss: 0.3579 - val_accuracy: 0.9209\n",
      "Epoch 106/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0719 - accuracy: 0.9724\n",
      "Epoch 106: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 2s 128ms/step - loss: 0.0714 - accuracy: 0.9728 - val_loss: 0.3774 - val_accuracy: 0.9209\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0596 - accuracy: 0.9783\n",
      "Epoch 107: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 3s 155ms/step - loss: 0.0596 - accuracy: 0.9783 - val_loss: 0.3910 - val_accuracy: 0.9281\n",
      "Epoch 108/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0421 - accuracy: 0.9853\n",
      "Epoch 108: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 3s 139ms/step - loss: 0.0416 - accuracy: 0.9855 - val_loss: 0.3962 - val_accuracy: 0.9281\n",
      "Epoch 109/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0384 - accuracy: 0.9835\n",
      "Epoch 109: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 2s 103ms/step - loss: 0.0378 - accuracy: 0.9837 - val_loss: 0.3763 - val_accuracy: 0.9424\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9855\n",
      "Epoch 110: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 2s 138ms/step - loss: 0.0357 - accuracy: 0.9855 - val_loss: 0.4172 - val_accuracy: 0.9281\n",
      "Epoch 111/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0427 - accuracy: 0.9779\n",
      "Epoch 111: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 2s 133ms/step - loss: 0.0426 - accuracy: 0.9783 - val_loss: 0.4093 - val_accuracy: 0.9209\n",
      "Epoch 112/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0272 - accuracy: 0.9963\n",
      "Epoch 112: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 2s 112ms/step - loss: 0.0270 - accuracy: 0.9964 - val_loss: 0.3736 - val_accuracy: 0.9353\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 0.9909\n",
      "Epoch 113: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 3s 183ms/step - loss: 0.0300 - accuracy: 0.9909 - val_loss: 0.2754 - val_accuracy: 0.9281\n",
      "Epoch 114/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0285 - accuracy: 0.9926\n",
      "Epoch 114: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 2s 135ms/step - loss: 0.0282 - accuracy: 0.9928 - val_loss: 0.5637 - val_accuracy: 0.9137\n",
      "Epoch 115/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0375 - accuracy: 0.9871\n",
      "Epoch 115: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 2s 113ms/step - loss: 0.0370 - accuracy: 0.9873 - val_loss: 0.4685 - val_accuracy: 0.9281\n",
      "Epoch 116/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0258 - accuracy: 0.9908\n",
      "Epoch 116: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 2s 114ms/step - loss: 0.0257 - accuracy: 0.9909 - val_loss: 0.4170 - val_accuracy: 0.9353\n",
      "Epoch 117/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0214 - accuracy: 0.9963\n",
      "Epoch 117: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 2s 125ms/step - loss: 0.0303 - accuracy: 0.9946 - val_loss: 0.3781 - val_accuracy: 0.9353\n",
      "Epoch 118/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0357 - accuracy: 0.9926\n",
      "Epoch 118: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 2s 120ms/step - loss: 0.0361 - accuracy: 0.9928 - val_loss: 0.2853 - val_accuracy: 0.9353\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0613 - accuracy: 0.9783\n",
      "Epoch 119: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 3s 182ms/step - loss: 0.0613 - accuracy: 0.9783 - val_loss: 0.3756 - val_accuracy: 0.9424\n",
      "Epoch 120/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0304 - accuracy: 0.9926\n",
      "Epoch 120: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 2s 130ms/step - loss: 0.0302 - accuracy: 0.9928 - val_loss: 0.3929 - val_accuracy: 0.9353\n",
      "Epoch 121/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0422 - accuracy: 0.9816\n",
      "Epoch 121: val_accuracy did not improve from 0.94245\n",
      "18/18 [==============================] - 2s 105ms/step - loss: 0.0416 - accuracy: 0.9819 - val_loss: 0.4856 - val_accuracy: 0.9281\n",
      "Epoch 122/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0290 - accuracy: 0.9963\n",
      "Epoch 122: val_accuracy improved from 0.94245 to 0.94964, saving model to lstm_model_32_s2_v1_5.keras\n",
      "18/18 [==============================] - 2s 141ms/step - loss: 0.0286 - accuracy: 0.9964 - val_loss: 0.3517 - val_accuracy: 0.9496\n",
      "Epoch 123/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0286 - accuracy: 0.9945\n",
      "Epoch 123: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 107ms/step - loss: 0.0282 - accuracy: 0.9946 - val_loss: 0.3745 - val_accuracy: 0.9424\n",
      "Epoch 124/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0241 - accuracy: 0.9926\n",
      "Epoch 124: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 122ms/step - loss: 0.0239 - accuracy: 0.9928 - val_loss: 0.3670 - val_accuracy: 0.9424\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0383 - accuracy: 0.9873\n",
      "Epoch 125: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 3s 187ms/step - loss: 0.0383 - accuracy: 0.9873 - val_loss: 0.3638 - val_accuracy: 0.9424\n",
      "Epoch 126/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0285 - accuracy: 0.9871\n",
      "Epoch 126: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 134ms/step - loss: 0.0281 - accuracy: 0.9873 - val_loss: 0.3472 - val_accuracy: 0.9353\n",
      "Epoch 127/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0275 - accuracy: 0.9908\n",
      "Epoch 127: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 114ms/step - loss: 0.0272 - accuracy: 0.9909 - val_loss: 0.4607 - val_accuracy: 0.9353\n",
      "Epoch 128/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0243 - accuracy: 0.9945\n",
      "Epoch 128: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 125ms/step - loss: 0.0240 - accuracy: 0.9946 - val_loss: 0.4601 - val_accuracy: 0.9281\n",
      "Epoch 129/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0319 - accuracy: 0.9835\n",
      "Epoch 129: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 105ms/step - loss: 0.0321 - accuracy: 0.9837 - val_loss: 0.5069 - val_accuracy: 0.9281\n",
      "Epoch 130/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0167 - accuracy: 0.9945\n",
      "Epoch 130: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 106ms/step - loss: 0.0166 - accuracy: 0.9946 - val_loss: 0.4775 - val_accuracy: 0.9281\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0447 - accuracy: 0.9855\n",
      "Epoch 131: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 3s 186ms/step - loss: 0.0447 - accuracy: 0.9855 - val_loss: 0.5198 - val_accuracy: 0.9281\n",
      "Epoch 132/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0591 - accuracy: 0.9816\n",
      "Epoch 132: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 118ms/step - loss: 0.0586 - accuracy: 0.9819 - val_loss: 0.4179 - val_accuracy: 0.9281\n",
      "Epoch 133/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0174 - accuracy: 0.9982\n",
      "Epoch 133: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 115ms/step - loss: 0.0188 - accuracy: 0.9964 - val_loss: 0.4051 - val_accuracy: 0.9281\n",
      "Epoch 134/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0289 - accuracy: 0.9871\n",
      "Epoch 134: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 123ms/step - loss: 0.0286 - accuracy: 0.9873 - val_loss: 0.4049 - val_accuracy: 0.9353\n",
      "Epoch 135/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0258 - accuracy: 0.9908\n",
      "Epoch 135: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 107ms/step - loss: 0.0263 - accuracy: 0.9909 - val_loss: 0.4197 - val_accuracy: 0.9424\n",
      "Epoch 136/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0420 - accuracy: 0.9890\n",
      "Epoch 136: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 117ms/step - loss: 0.0417 - accuracy: 0.9891 - val_loss: 0.4691 - val_accuracy: 0.9353\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9964\n",
      "Epoch 137: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 3s 179ms/step - loss: 0.0174 - accuracy: 0.9964 - val_loss: 0.5288 - val_accuracy: 0.9209\n",
      "Epoch 138/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0238 - accuracy: 0.9926\n",
      "Epoch 138: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 125ms/step - loss: 0.0235 - accuracy: 0.9928 - val_loss: 0.3849 - val_accuracy: 0.9353\n",
      "Epoch 139/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0182 - accuracy: 0.9982\n",
      "Epoch 139: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 125ms/step - loss: 0.0180 - accuracy: 0.9982 - val_loss: 0.5057 - val_accuracy: 0.9281\n",
      "Epoch 140/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0193 - accuracy: 0.9908\n",
      "Epoch 140: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 114ms/step - loss: 0.0191 - accuracy: 0.9909 - val_loss: 0.4081 - val_accuracy: 0.9424\n",
      "Epoch 141/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0365 - accuracy: 0.9871\n",
      "Epoch 141: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 113ms/step - loss: 0.0360 - accuracy: 0.9873 - val_loss: 0.6245 - val_accuracy: 0.9065\n",
      "Epoch 142/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0240 - accuracy: 0.9908\n",
      "Epoch 142: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 123ms/step - loss: 0.0238 - accuracy: 0.9909 - val_loss: 0.4805 - val_accuracy: 0.9353\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9909\n",
      "Epoch 143: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 3s 166ms/step - loss: 0.0264 - accuracy: 0.9909 - val_loss: 0.4934 - val_accuracy: 0.9281\n",
      "Epoch 144/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0255 - accuracy: 0.9890\n",
      "Epoch 144: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 3s 152ms/step - loss: 0.0252 - accuracy: 0.9891 - val_loss: 0.3945 - val_accuracy: 0.9424\n",
      "Epoch 145/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0285 - accuracy: 0.9908\n",
      "Epoch 145: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 117ms/step - loss: 0.0281 - accuracy: 0.9909 - val_loss: 0.4104 - val_accuracy: 0.9424\n",
      "Epoch 146/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0205 - accuracy: 0.9926\n",
      "Epoch 146: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 109ms/step - loss: 0.0202 - accuracy: 0.9928 - val_loss: 0.4043 - val_accuracy: 0.9353\n",
      "Epoch 147/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0125 - accuracy: 0.9982\n",
      "Epoch 147: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 121ms/step - loss: 0.0123 - accuracy: 0.9982 - val_loss: 0.5154 - val_accuracy: 0.9209\n",
      "Epoch 148/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0103 - accuracy: 0.9982\n",
      "Epoch 148: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 114ms/step - loss: 0.0102 - accuracy: 0.9982 - val_loss: 0.4133 - val_accuracy: 0.9424\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9946\n",
      "Epoch 149: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 3s 171ms/step - loss: 0.0317 - accuracy: 0.9946 - val_loss: 0.4412 - val_accuracy: 0.9424\n",
      "Epoch 150/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0127 - accuracy: 0.9963\n",
      "Epoch 150: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 3s 148ms/step - loss: 0.0138 - accuracy: 0.9964 - val_loss: 0.5631 - val_accuracy: 0.9209\n",
      "Epoch 151/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0170 - accuracy: 0.9945\n",
      "Epoch 151: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 109ms/step - loss: 0.0168 - accuracy: 0.9946 - val_loss: 0.5097 - val_accuracy: 0.9209\n",
      "Epoch 152/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0386 - accuracy: 0.9890\n",
      "Epoch 152: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 102ms/step - loss: 0.0421 - accuracy: 0.9873 - val_loss: 0.5125 - val_accuracy: 0.9209\n",
      "Epoch 153/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2428 - accuracy: 0.9449\n",
      "Epoch 153: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 102ms/step - loss: 0.2401 - accuracy: 0.9457 - val_loss: 0.4422 - val_accuracy: 0.9137\n",
      "Epoch 154/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1183 - accuracy: 0.9651\n",
      "Epoch 154: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 102ms/step - loss: 0.1167 - accuracy: 0.9656 - val_loss: 0.3919 - val_accuracy: 0.9424\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.9783\n",
      "Epoch 155: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 3s 145ms/step - loss: 0.0631 - accuracy: 0.9783 - val_loss: 0.6076 - val_accuracy: 0.8993\n",
      "Epoch 156/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1942 - accuracy: 0.9301\n",
      "Epoch 156: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 0.1914 - accuracy: 0.9312 - val_loss: 0.7607 - val_accuracy: 0.8849\n",
      "Epoch 157/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1017 - accuracy: 0.9724\n",
      "Epoch 157: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 103ms/step - loss: 0.1028 - accuracy: 0.9710 - val_loss: 0.3032 - val_accuracy: 0.9281\n",
      "Epoch 158/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0437 - accuracy: 0.9835\n",
      "Epoch 158: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 110ms/step - loss: 0.0433 - accuracy: 0.9837 - val_loss: 0.3794 - val_accuracy: 0.9353\n",
      "Epoch 159/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0256 - accuracy: 0.9908\n",
      "Epoch 159: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 114ms/step - loss: 0.0253 - accuracy: 0.9909 - val_loss: 0.3600 - val_accuracy: 0.9424\n",
      "Epoch 160/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0198 - accuracy: 0.9982\n",
      "Epoch 160: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 112ms/step - loss: 0.0197 - accuracy: 0.9982 - val_loss: 0.3975 - val_accuracy: 0.9353\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9946\n",
      "Epoch 161: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 130ms/step - loss: 0.0191 - accuracy: 0.9946 - val_loss: 0.3821 - val_accuracy: 0.9424\n",
      "Epoch 162/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9891\n",
      "Epoch 162: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 4s 199ms/step - loss: 0.0318 - accuracy: 0.9891 - val_loss: 0.4681 - val_accuracy: 0.9209\n",
      "Epoch 163/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0184 - accuracy: 0.9908\n",
      "Epoch 163: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 133ms/step - loss: 0.0182 - accuracy: 0.9909 - val_loss: 0.3806 - val_accuracy: 0.9353\n",
      "Epoch 164/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0219 - accuracy: 0.9926\n",
      "Epoch 164: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 112ms/step - loss: 0.0217 - accuracy: 0.9928 - val_loss: 0.4242 - val_accuracy: 0.9281\n",
      "Epoch 165/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0132 - accuracy: 0.9982\n",
      "Epoch 165: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 114ms/step - loss: 0.0131 - accuracy: 0.9982 - val_loss: 0.4139 - val_accuracy: 0.9281\n",
      "Epoch 166/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0137 - accuracy: 0.9982\n",
      "Epoch 166: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 124ms/step - loss: 0.0137 - accuracy: 0.9982 - val_loss: 0.4373 - val_accuracy: 0.9281\n",
      "Epoch 167/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9928\n",
      "Epoch 167: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 3s 146ms/step - loss: 0.0174 - accuracy: 0.9928 - val_loss: 0.4633 - val_accuracy: 0.9209\n",
      "Epoch 168/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0237 - accuracy: 0.9908\n",
      "Epoch 168: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 3s 175ms/step - loss: 0.0234 - accuracy: 0.9909 - val_loss: 0.3797 - val_accuracy: 0.9209\n",
      "Epoch 169/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0261 - accuracy: 0.9908\n",
      "Epoch 169: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 111ms/step - loss: 0.0258 - accuracy: 0.9909 - val_loss: 0.5306 - val_accuracy: 0.9209\n",
      "Epoch 170/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0183 - accuracy: 0.9963\n",
      "Epoch 170: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 133ms/step - loss: 0.0181 - accuracy: 0.9964 - val_loss: 0.4028 - val_accuracy: 0.9281\n",
      "Epoch 171/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0108 - accuracy: 0.9982\n",
      "Epoch 171: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 104ms/step - loss: 0.0107 - accuracy: 0.9982 - val_loss: 0.4821 - val_accuracy: 0.9353\n",
      "Epoch 172/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0127 - accuracy: 0.9963\n",
      "Epoch 172: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 109ms/step - loss: 0.0136 - accuracy: 0.9964 - val_loss: 0.3871 - val_accuracy: 0.9281\n",
      "Epoch 173/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9946\n",
      "Epoch 173: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 131ms/step - loss: 0.0172 - accuracy: 0.9946 - val_loss: 0.4802 - val_accuracy: 0.9137\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9946\n",
      "Epoch 174: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 3s 178ms/step - loss: 0.0140 - accuracy: 0.9946 - val_loss: 0.5024 - val_accuracy: 0.9353\n",
      "Epoch 175/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0140 - accuracy: 0.9963\n",
      "Epoch 175: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 132ms/step - loss: 0.0138 - accuracy: 0.9964 - val_loss: 0.4489 - val_accuracy: 0.9353\n",
      "Epoch 176/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0118 - accuracy: 0.9963\n",
      "Epoch 176: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 119ms/step - loss: 0.0119 - accuracy: 0.9964 - val_loss: 0.4536 - val_accuracy: 0.9353\n",
      "Epoch 177/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0110 - accuracy: 0.9963\n",
      "Epoch 177: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 113ms/step - loss: 0.0111 - accuracy: 0.9964 - val_loss: 0.4489 - val_accuracy: 0.9281\n",
      "Epoch 178/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0127 - accuracy: 0.9945\n",
      "Epoch 178: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 116ms/step - loss: 0.0126 - accuracy: 0.9946 - val_loss: 0.4911 - val_accuracy: 0.9353\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9909\n",
      "Epoch 179: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 3s 152ms/step - loss: 0.0290 - accuracy: 0.9909 - val_loss: 0.7464 - val_accuracy: 0.8777\n",
      "Epoch 180/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0533 - accuracy: 0.9816\n",
      "Epoch 180: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 3s 169ms/step - loss: 0.0525 - accuracy: 0.9819 - val_loss: 0.4721 - val_accuracy: 0.9281\n",
      "Epoch 181/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0255 - accuracy: 0.9945\n",
      "Epoch 181: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 113ms/step - loss: 0.0251 - accuracy: 0.9946 - val_loss: 0.4381 - val_accuracy: 0.9424\n",
      "Epoch 182/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0073 - accuracy: 0.9982\n",
      "Epoch 182: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 123ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.4582 - val_accuracy: 0.9353\n",
      "Epoch 183/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0161 - accuracy: 0.9945\n",
      "Epoch 183: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 113ms/step - loss: 0.0159 - accuracy: 0.9946 - val_loss: 0.4841 - val_accuracy: 0.9353\n",
      "Epoch 184/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0101 - accuracy: 0.9963\n",
      "Epoch 184: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 132ms/step - loss: 0.0100 - accuracy: 0.9964 - val_loss: 0.4691 - val_accuracy: 0.9353\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9964\n",
      "Epoch 185: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 3s 175ms/step - loss: 0.0140 - accuracy: 0.9964 - val_loss: 0.4208 - val_accuracy: 0.9281\n",
      "Epoch 186/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0164 - accuracy: 0.9926\n",
      "Epoch 186: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 3s 160ms/step - loss: 0.0162 - accuracy: 0.9928 - val_loss: 0.5890 - val_accuracy: 0.9209\n",
      "Epoch 187/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0153 - accuracy: 0.9926\n",
      "Epoch 187: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 117ms/step - loss: 0.0151 - accuracy: 0.9928 - val_loss: 0.3849 - val_accuracy: 0.9353\n",
      "Epoch 188/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0089 - accuracy: 0.9945\n",
      "Epoch 188: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 105ms/step - loss: 0.0088 - accuracy: 0.9946 - val_loss: 0.4848 - val_accuracy: 0.9353\n",
      "Epoch 189/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 189: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 104ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.4506 - val_accuracy: 0.9424\n",
      "Epoch 190/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0120 - accuracy: 0.9963\n",
      "Epoch 190: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 105ms/step - loss: 0.0119 - accuracy: 0.9964 - val_loss: 0.4697 - val_accuracy: 0.9424\n",
      "Epoch 191/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9964\n",
      "Epoch 191: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 139ms/step - loss: 0.0086 - accuracy: 0.9964 - val_loss: 0.4898 - val_accuracy: 0.9353\n",
      "Epoch 192/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0181 - accuracy: 0.9908\n",
      "Epoch 192: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 3s 158ms/step - loss: 0.0179 - accuracy: 0.9909 - val_loss: 0.4500 - val_accuracy: 0.9353\n",
      "Epoch 193/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0284 - accuracy: 0.9890\n",
      "Epoch 193: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 105ms/step - loss: 0.0319 - accuracy: 0.9873 - val_loss: 0.4426 - val_accuracy: 0.9281\n",
      "Epoch 194/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0393 - accuracy: 0.9871\n",
      "Epoch 194: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 106ms/step - loss: 0.0388 - accuracy: 0.9873 - val_loss: 0.7696 - val_accuracy: 0.8777\n",
      "Epoch 195/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0290 - accuracy: 0.9890\n",
      "Epoch 195: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 130ms/step - loss: 0.0296 - accuracy: 0.9891 - val_loss: 0.3983 - val_accuracy: 0.9281\n",
      "Epoch 196/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0141 - accuracy: 0.9963\n",
      "Epoch 196: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 132ms/step - loss: 0.0140 - accuracy: 0.9964 - val_loss: 0.4579 - val_accuracy: 0.9281\n",
      "Epoch 197/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9928\n",
      "Epoch 197: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 3s 146ms/step - loss: 0.0171 - accuracy: 0.9928 - val_loss: 0.4431 - val_accuracy: 0.9281\n",
      "Epoch 198/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0331 - accuracy: 0.9890\n",
      "Epoch 198: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 3s 161ms/step - loss: 0.0336 - accuracy: 0.9891 - val_loss: 0.5935 - val_accuracy: 0.9209\n",
      "Epoch 199/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0322 - accuracy: 0.9890\n",
      "Epoch 199: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 107ms/step - loss: 0.0317 - accuracy: 0.9891 - val_loss: 0.5752 - val_accuracy: 0.9281\n",
      "Epoch 200/200\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0119 - accuracy: 0.9982\n",
      "Epoch 200: val_accuracy did not improve from 0.94964\n",
      "18/18 [==============================] - 2s 105ms/step - loss: 0.0118 - accuracy: 0.9982 - val_loss: 0.6290 - val_accuracy: 0.9281\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7b433c098670>"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "# Print the model summary\n",
    "# model.summary()\n",
    "\n",
    "# Train the model with your dataset\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "itertion +=1\n",
    "\n",
    "# Specify the filepath for saving the best model\n",
    "checkpoint_filepath = f'lstm_model_32_s2_v1_{itertion}.keras'\n",
    "\n",
    "# Create ModelCheckpoint callback\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_best_only=True,\n",
    "    monitor='val_accuracy',  # You can change this to 'val_loss' or another metric\n",
    "    mode='max',  # 'max' for accuracy, 'min' for loss\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Assuming X_train, y_train, X_test, y_test are your training and testing data\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=200,\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    callbacks=[model_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "GN4GsmWwPq6A"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyNo84QR68dhMl/u+ctoqT3+"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
